{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtqtUPVLwdY2uqnrTJai1c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManJ-PC/introducao-ao-pln-com-python/blob/master/Contents_of_descriptions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **Bag of Words**\n",
        "\n",
        "BoW é uma forma de representar o texto de acordo com a ocorrência das palavras nele. Traduzindo para o português, o \"saco de palavras\" recebe esse nome porque não leva em conta a ordem ou a estrutura das palavras no texto, apenas se ela aparece ou a frequência com que aparece nele. Por exemplo, se a palavra TURING aparece muito num texto, ela se torna mais central e importante para a máquina. Portanto, BoW pode ser um ótimo método para determinar as palavras significativas de um texto com base no número de vezes que ela é usada. Bem simples, né? Basicamente, para gerar um modelo de bag of words precisamos realizar três passos:\n",
        "\n",
        "\n",
        "1.  Selecionar os dados\n",
        "2.  Gerar o vocabulário\n",
        "3.  Formar vetores a partir do documento \n",
        "\n",
        "Vamos ver cada um desses passos separadamente:\n",
        "\n",
        "## Selecionando seus dados\n",
        "\n",
        "Essa parte diz respeito a selecionar os dados que serão usados - no nosso caso, o texto - e prepará-lo de forma que a máquina consiga processá-lo bem. Então, primeiro, vamos falar sobre os dados com qual trabalharemos nesse artigo. Nosso texto será descrição do anúncio \"Quinta T14 de luxo à venda em Marco de Canaveses\" do [site](https://www.remax.pt/imoveis/venda-quinta-t14-marco-de-canaveses-penhalonga-e-pacos-de-gaiolo/126021010-57) da Remax:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zZZ1ACyfE0rH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ufTLFqyB3uPB"
      },
      "outputs": [],
      "source": [
        "description = \"\"\"Solar histórico conhecido por “Quinta ou Casa do Carrapatelo”. Datada de 1706, é constituída por uma bonita casa senhorial com brasão, capela, adega, celeiro, casa de animais, espigueiro, 7 minas de água, num total de cerca de 2,5 hectares.\n",
        "\n",
        "Trata-se de um imóvel com raízes históricas, sendo aludida em diversos livros da especialidade, assim como em diversos sites acerca da região.\n",
        "\n",
        "A propriedade,na sua totalidade, é composta pelos seguintes artigos:\n",
        "\n",
        "Terrenos de cultura, ramada, oliveiras, fruteiras e dependências agrícolas, com a área de 15.300 m2;\n",
        "Prédio urbanos, compostos de casa de habitação com capela e quintais, tendo anexo uma casa de caseiro, com a área coberta de 544 m2 e total do terreno de 5.212m2;\n",
        "Prédio urbano, composto de casa de habitação de caseiros, com 2 pisos, com a área de 63,80 m2 por piso totalizando 127,60 m2;\n",
        "a área total de cosntrução será de apropximadamente 1,200m2\n",
        "Nota: medidas conforme registos. Levatamento topográfico disponivel.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pré-processamento\n",
        "\n",
        "Com nossos dados escolhidos precisamos prepará-los. Chamamos isso de pré-processamento e você já sabe que pré-processar um texto é essencial quando queremos trabalhar com ele, principalmente quando queremos aplicar um modelo de predição ou outros métodos estatísticos. São muitos os métodos de pré-processamento, mas aqui vamos realizar apenas alguns:\n",
        "\n",
        "* Colocar todas as letras em minúsculo\n",
        "* Selecionar apenas letras com regex\n",
        "* Juntar os tokens em um texto - já que, ao usar a função .findall do regex, nosso texto é dividido em tokens, ou seja, cada palavra vira uma string individual em uma lista"
      ],
      "metadata": {
        "id": "O0idU0ugIrFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "#deixa todas as letras minúsculas\n",
        "description_min = description.lower()"
      ],
      "metadata": {
        "id": "5MQFHHQa8nMe"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seleciona apenas letras (lembrando que o texto está em português e as letras possuem acento): In the regular expression pattern r'[a-zéóáêâãõç]+', a-zéóáêâãõç represents a range of characters from \"a\" to \"z\" including the letters \"é\", \"ó\", \"á\", \"ê\", \"â\", \"ã\", \"õ\", and \"ç\". The + quantifier means that one or more occurrences of these characters should be matched.\n",
        "apenas_letras\n"
      ],
      "metadata": {
        "id": "-wiPjfWukd3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ec321dd-f9b1-43b5-84e8-b2be174273cd"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['solar',\n",
              " 'histórico',\n",
              " 'conhecido',\n",
              " 'por',\n",
              " 'quinta',\n",
              " 'ou',\n",
              " 'casa',\n",
              " 'do',\n",
              " 'carrapatelo',\n",
              " 'datada',\n",
              " 'de',\n",
              " 'é',\n",
              " 'constitu',\n",
              " 'da',\n",
              " 'por',\n",
              " 'uma',\n",
              " 'bonita',\n",
              " 'casa',\n",
              " 'senhorial',\n",
              " 'com',\n",
              " 'brasão',\n",
              " 'capela',\n",
              " 'adega',\n",
              " 'celeiro',\n",
              " 'casa',\n",
              " 'de',\n",
              " 'animais',\n",
              " 'espigueiro',\n",
              " 'minas',\n",
              " 'de',\n",
              " 'água',\n",
              " 'num',\n",
              " 'total',\n",
              " 'de',\n",
              " 'cerca',\n",
              " 'de',\n",
              " 'hectares',\n",
              " 'trata',\n",
              " 'se',\n",
              " 'de',\n",
              " 'um',\n",
              " 'imóvel',\n",
              " 'com',\n",
              " 'ra',\n",
              " 'zes',\n",
              " 'históricas',\n",
              " 'sendo',\n",
              " 'aludida',\n",
              " 'em',\n",
              " 'diversos',\n",
              " 'livros',\n",
              " 'da',\n",
              " 'especialidade',\n",
              " 'assim',\n",
              " 'como',\n",
              " 'em',\n",
              " 'diversos',\n",
              " 'sites',\n",
              " 'acerca',\n",
              " 'da',\n",
              " 'região',\n",
              " 'a',\n",
              " 'propriedade',\n",
              " 'na',\n",
              " 'sua',\n",
              " 'totalidade',\n",
              " 'é',\n",
              " 'composta',\n",
              " 'pelos',\n",
              " 'seguintes',\n",
              " 'artigos',\n",
              " 'terrenos',\n",
              " 'de',\n",
              " 'cultura',\n",
              " 'ramada',\n",
              " 'oliveiras',\n",
              " 'fruteiras',\n",
              " 'e',\n",
              " 'dependências',\n",
              " 'agr',\n",
              " 'colas',\n",
              " 'com',\n",
              " 'a',\n",
              " 'área',\n",
              " 'de',\n",
              " 'm',\n",
              " 'prédio',\n",
              " 'urbanos',\n",
              " 'compostos',\n",
              " 'de',\n",
              " 'casa',\n",
              " 'de',\n",
              " 'habitação',\n",
              " 'com',\n",
              " 'capela',\n",
              " 'e',\n",
              " 'quintais',\n",
              " 'tendo',\n",
              " 'anexo',\n",
              " 'uma',\n",
              " 'casa',\n",
              " 'de',\n",
              " 'caseiro',\n",
              " 'com',\n",
              " 'a',\n",
              " 'área',\n",
              " 'coberta',\n",
              " 'de',\n",
              " 'm',\n",
              " 'e',\n",
              " 'total',\n",
              " 'do',\n",
              " 'terreno',\n",
              " 'de',\n",
              " 'm',\n",
              " 'prédio',\n",
              " 'urbano',\n",
              " 'composto',\n",
              " 'de',\n",
              " 'casa',\n",
              " 'de',\n",
              " 'habitação',\n",
              " 'de',\n",
              " 'caseiros',\n",
              " 'com',\n",
              " 'pisos',\n",
              " 'com',\n",
              " 'a',\n",
              " 'área',\n",
              " 'de',\n",
              " 'm',\n",
              " 'por',\n",
              " 'piso',\n",
              " 'totalizando',\n",
              " 'm',\n",
              " 'a',\n",
              " 'área',\n",
              " 'total',\n",
              " 'de',\n",
              " 'cosntrução',\n",
              " 'será',\n",
              " 'de',\n",
              " 'apropximadamente',\n",
              " 'm',\n",
              " 'nota',\n",
              " 'medidas',\n",
              " 'conforme',\n",
              " 'registos',\n",
              " 'levatamento',\n",
              " 'topográfico',\n",
              " 'disponivel']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#junta o texto, já que o .findall separa em tokens\n",
        "new_description = \" \".join(apenas_letras)\n",
        "new_description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "tt-_xdi0kk-P",
        "outputId": "95b73862-8bdf-4982-aed6-1751d8147f58"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'solar histórico conhecido por quinta ou casa do carrapatelo datada de é constitu da por uma bonita casa senhorial com brasão capela adega celeiro casa de animais espigueiro minas de água num total de cerca de hectares trata se de um imóvel com ra zes históricas sendo aludida em diversos livros da especialidade assim como em diversos sites acerca da região a propriedade na sua totalidade é composta pelos seguintes artigos terrenos de cultura ramada oliveiras fruteiras e dependências agr colas com a área de m prédio urbanos compostos de casa de habitação com capela e quintais tendo anexo uma casa de caseiro com a área coberta de m e total do terreno de m prédio urbano composto de casa de habitação de caseiros com pisos com a área de m por piso totalizando m a área total de cosntrução será de apropximadamente m nota medidas conforme registos levatamento topográfico disponivel'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gerando o vocabulário\n",
        "Já pensou se você conseguisse memorizar todas as palavras que você visse? Diferente de nós que precisamos ver, rever uma, duas e provavelmente mais vezes para memorizar, o computador já consegue fazer isso de primeira. Então para gerar o vocabulário que nada mais é que a coleção de todas as palavras que ocorrem em um texto, basta passarmos todas elas uma vez só. Então a ideia para o código é basicamente:\n",
        "\n",
        "* Separar nosso texto em tokens;\n",
        "* Criar uma lista para guardarmos o vocabulário;\n",
        "* Fazer um loop para percorrer o texto inteiro;\n",
        "* Criar uma condicional para verificar se a palavra está na lista -fazemos isso porque nosso vocabulário conta apenas as ocorrências únicas, sem repetições de palavras;\n",
        "* Caso não esteja, ela é adicionada.\n",
        "\n",
        "Vamos ver como fica isso na prática? Olha só!"
      ],
      "metadata": {
        "id": "ERf0iuLTKS7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn0fQkdhCTuz",
        "outputId": "ab7319f2-02a6-4b50-bbac-00b22b078a88"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(new_description)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QcWRxqjCfhk",
        "outputId": "ea559e69-e3a5-4557-af96-d03f65a33e4d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['solar',\n",
              " 'histórico',\n",
              " 'conhecido',\n",
              " 'por',\n",
              " 'quinta',\n",
              " 'ou',\n",
              " 'casa',\n",
              " 'do',\n",
              " 'carrapatelo',\n",
              " 'datada',\n",
              " 'de',\n",
              " 'é',\n",
              " 'constitu',\n",
              " 'da',\n",
              " 'por',\n",
              " 'uma',\n",
              " 'bonita',\n",
              " 'casa',\n",
              " 'senhorial',\n",
              " 'com',\n",
              " 'brasão',\n",
              " 'capela',\n",
              " 'adega',\n",
              " 'celeiro',\n",
              " 'casa',\n",
              " 'de',\n",
              " 'animais',\n",
              " 'espigueiro',\n",
              " 'minas',\n",
              " 'de',\n",
              " 'água',\n",
              " 'num',\n",
              " 'total',\n",
              " 'de',\n",
              " 'cerca',\n",
              " 'de',\n",
              " 'hectares',\n",
              " 'trata',\n",
              " 'se',\n",
              " 'de',\n",
              " 'um',\n",
              " 'imóvel',\n",
              " 'com',\n",
              " 'ra',\n",
              " 'zes',\n",
              " 'históricas',\n",
              " 'sendo',\n",
              " 'aludida',\n",
              " 'em',\n",
              " 'diversos',\n",
              " 'livros',\n",
              " 'da',\n",
              " 'especialidade',\n",
              " 'assim',\n",
              " 'como',\n",
              " 'em',\n",
              " 'diversos',\n",
              " 'sites',\n",
              " 'acerca',\n",
              " 'da',\n",
              " 'região',\n",
              " 'a',\n",
              " 'propriedade',\n",
              " 'na',\n",
              " 'sua',\n",
              " 'totalidade',\n",
              " 'é',\n",
              " 'composta',\n",
              " 'pelos',\n",
              " 'seguintes',\n",
              " 'artigos',\n",
              " 'terrenos',\n",
              " 'de',\n",
              " 'cultura',\n",
              " 'ramada',\n",
              " 'oliveiras',\n",
              " 'fruteiras',\n",
              " 'e',\n",
              " 'dependências',\n",
              " 'agr',\n",
              " 'colas',\n",
              " 'com',\n",
              " 'a',\n",
              " 'área',\n",
              " 'de',\n",
              " 'm',\n",
              " 'prédio',\n",
              " 'urbanos',\n",
              " 'compostos',\n",
              " 'de',\n",
              " 'casa',\n",
              " 'de',\n",
              " 'habitação',\n",
              " 'com',\n",
              " 'capela',\n",
              " 'e',\n",
              " 'quintais',\n",
              " 'tendo',\n",
              " 'anexo',\n",
              " 'uma',\n",
              " 'casa',\n",
              " 'de',\n",
              " 'caseiro',\n",
              " 'com',\n",
              " 'a',\n",
              " 'área',\n",
              " 'coberta',\n",
              " 'de',\n",
              " 'm',\n",
              " 'e',\n",
              " 'total',\n",
              " 'do',\n",
              " 'terreno',\n",
              " 'de',\n",
              " 'm',\n",
              " 'prédio',\n",
              " 'urbano',\n",
              " 'composto',\n",
              " 'de',\n",
              " 'casa',\n",
              " 'de',\n",
              " 'habitação',\n",
              " 'de',\n",
              " 'caseiros',\n",
              " 'com',\n",
              " 'pisos',\n",
              " 'com',\n",
              " 'a',\n",
              " 'área',\n",
              " 'de',\n",
              " 'm',\n",
              " 'por',\n",
              " 'piso',\n",
              " 'totalizando',\n",
              " 'm',\n",
              " 'a',\n",
              " 'área',\n",
              " 'total',\n",
              " 'de',\n",
              " 'cosntrução',\n",
              " 'será',\n",
              " 'de',\n",
              " 'apropximadamente',\n",
              " 'm',\n",
              " 'nota',\n",
              " 'medidas',\n",
              " 'conforme',\n",
              " 'registos',\n",
              " 'levatamento',\n",
              " 'topográfico',\n",
              " 'disponivel']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulário"
      ],
      "metadata": {
        "id": "Gy2YM9T5DGTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Vocab = []\n",
        "for token in tokens:\n",
        "  if token not in Vocab:\n",
        "    Vocab.append(token)\n",
        "\n",
        "print(Vocab, \"\\n\", len(Vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGyHDk9NCvgl",
        "outputId": "d1f47f5f-8f71-469b-e247-3e975411f6ee"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['solar', 'histórico', 'conhecido', 'por', 'quinta', 'ou', 'casa', 'do', 'carrapatelo', 'datada', 'de', 'é', 'constitu', 'da', 'uma', 'bonita', 'senhorial', 'com', 'brasão', 'capela', 'adega', 'celeiro', 'animais', 'espigueiro', 'minas', 'água', 'num', 'total', 'cerca', 'hectares', 'trata', 'se', 'um', 'imóvel', 'ra', 'zes', 'históricas', 'sendo', 'aludida', 'em', 'diversos', 'livros', 'especialidade', 'assim', 'como', 'sites', 'acerca', 'região', 'a', 'propriedade', 'na', 'sua', 'totalidade', 'composta', 'pelos', 'seguintes', 'artigos', 'terrenos', 'cultura', 'ramada', 'oliveiras', 'fruteiras', 'e', 'dependências', 'agr', 'colas', 'área', 'm', 'prédio', 'urbanos', 'compostos', 'habitação', 'quintais', 'tendo', 'anexo', 'caseiro', 'coberta', 'terreno', 'urbano', 'composto', 'caseiros', 'pisos', 'piso', 'totalizando', 'cosntrução', 'será', 'apropximadamente', 'nota', 'medidas', 'conforme', 'registos', 'levatamento', 'topográfico', 'disponivel'] \n",
            " 94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Formando vetor do documento\n",
        "\n",
        "Como o vocabulário tem um número fixo de palavras, podemos usar uma representação de tamanho fixo equivalente a esse número: um vetor! Cada elemento desse vetor corresponderá a uma palavra do vocabulário. Há diversas formas de preencher nosso vetor com números para representar um documento (uma 'amostra' de um conjunto maior de textos), por exemplo, usar a contagem de vezes em que a palavra aparece nele. Mas a maneira mais básica de fazer isso é atribuindo um valor booleano: 1 se a palavra aparece, 0 se não; isso é chamado de one-hot encoding.\n",
        "\n",
        "Podemos pensar nesse processo como uma tabulação do documento, não veja o exemplo com as frases \"Solar histórico conhecido por “Quinta ou Casa do Carrapatelo”. Datada de 1706, é constituída por uma bonita casa senhorial com brasão, capela, adega, celeiro, casa de animais, espigueiro, 7 minas de água, num total de cerca de 2,5 hectares.\":"
      ],
      "metadata": {
        "id": "GZjXiWN-MLc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assim, para codificar com one-hot, o passo a passo é:\n",
        "\n",
        "* Criar uma lista que representa o vetor;\n",
        "* Fazer um loop para percorrer todas as palavras do vocabulário;\n",
        "* Se a palavra estiver no documento, adicionar 1 à lista; caso contrário, adicionar 0;\n",
        "* Transformar a lista final em um array do numpy e retornar."
      ],
      "metadata": {
        "id": "YFb5bPnOPt1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "xVp6CzB-PRRk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cria_vetor_documento(documento, vocab):\n",
        "  vetor = []\n",
        "\n",
        "  for palavra in vocab:\n",
        "    if palavra in documento:\n",
        "      vetor.append(1)\n",
        "    else:\n",
        "      vetor.append(0)\n",
        "  return np.array(vector)"
      ],
      "metadata": {
        "id": "VGAyshRoPT7M"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TFIDF**\n",
        "\n",
        "TF-IDF vem para superar os problemas do Bag of Words. Trata-se de medidas estatísticas para medir o quão importante uma palavra é em um documento (texto), assim como BoW, mas com algumas diferenças. Com ele, podemos perceber a importância de uma palavra por meio de uma pontuação, o TF-IDF de uma palavra em um texto é feito multiplicando duas métricas diferentes:\n",
        "\n",
        "* Term Frequency (a frequência do termo), que mede a frequência com que um termo ocorre num documento;\n",
        "* Inverse Document Frequency (inverso da frequência nos documentos), que mede o quão importante um termo é no contexto de todos os documentos."
      ],
      "metadata": {
        "id": "C2qBxjT6Pxg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Em outras palavras, para o TF-IDF, quanto mais frequente uma palavra é em seu documento, mais importante ela tende a ser. Entretanto, isso depende da repetição dela ao longo de todos os documentos que estão sendo analisados.\n",
        "\n",
        "Por exemplo, suponhamos que estejamos analisando três documentos:\n",
        "\n",
        "* uma revista de futebol,\n",
        "* uma de vôlei\n",
        "* e uma de basquete.\n",
        "Temos palavras que se repetem ao longo de todos esses documentos, por exemplo, a palavra \"esporte\" deve aparecer em todas as três revistas, certo? Então, provavelmente, não contribui muito para uma análise. Porém, palavras que se repetem muito em documentos individuais dizem mais a respeito dele, então a palavra \"cesta\", por exemplo, que pode se repetir muito na revista sobre basquete, mas não nas outras, tende a se tornar mais importante para o TF-IDF. Por isso dizemos que a repetição das palavras importa com relação aos documentos que estão sendo analisados. Interessante, né?\n",
        "\n",
        "No nosso anúncio, consideraremos cada paragrafo como um documento (mas poderíamos fazer de outras formas também, considerando cada frase como um documento, por exemplo)."
      ],
      "metadata": {
        "id": "jEW5VlJ9R7h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragrafo1 = \"\"\"Solar histórico conhecido por Quinta ou Casa do Carrapatelo. Datada de 1706, é constituída por uma bonita casa senhorial com brasão, capela, adega, celeiro, casa de animais, espigueiro, 7 minas de água, num total de cerca de 2,5 hectares.\"\"\".lower()\n",
        "paragrafo2 = \"\"\"Trata-se de um imóvel com raízes históricas, sendo aludida em diversos livros da especialidade, assim como em diversos sites acerca da região.\"\"\".lower()"
      ],
      "metadata": {
        "id": "prZFW-stSDXu"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementando\n",
        "\n",
        "Para a implementação de TF-IDF precisamos seguir passos muito semelhantes ao que fizemos com BoW anteriormente:\n",
        "\n",
        "* Primeiro, é preciso selecionar seus dados e pré-processar seu texto;\n",
        "* Depois, gerar um vocabulário, ou seja, uma lista com todos os termos do nosso texto;\n",
        "Essas duas etapas já foram feitas lá em cima, então não vamos repeti-la - Lembre-se apenas que nosso vocabulário está armazenado na lista Vocab - Portanto, vamos direto para o último passo:\n",
        "\n",
        "* Gerar um dicionário de frequência desses termos\n",
        "Então vamos para a implementação!\n",
        "\n",
        "Para criar o dicionário vamos criar uma função dicionario_de_contagem, que retorna a palavra e a quantidade de ocorrências dela. Basicamente, essa função recebe como argumento a lista Vocab, que já criamos, e o documento tokenizado, e cria um dicionário com as palavras do poema seguidas do número de ocorrências delas:"
      ],
      "metadata": {
        "id": "EUDMYt3zTqK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p1_tokens = paragrafo1.split()\n",
        "\n",
        "p2_tokens = paragrafo2.split()\n",
        "\n",
        "p1_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7cchT29Xbmw",
        "outputId": "75bc36d4-fae6-4cef-eb70-33e7c548abd6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['solar',\n",
              " 'histórico',\n",
              " 'conhecido',\n",
              " 'por',\n",
              " 'quinta',\n",
              " 'ou',\n",
              " 'casa',\n",
              " 'do',\n",
              " 'carrapatelo.',\n",
              " 'datada',\n",
              " 'de',\n",
              " '1706,',\n",
              " 'é',\n",
              " 'constituída',\n",
              " 'por',\n",
              " 'uma',\n",
              " 'bonita',\n",
              " 'casa',\n",
              " 'senhorial',\n",
              " 'com',\n",
              " 'brasão,',\n",
              " 'capela,',\n",
              " 'adega,',\n",
              " 'celeiro,',\n",
              " 'casa',\n",
              " 'de',\n",
              " 'animais,',\n",
              " 'espigueiro,',\n",
              " '7',\n",
              " 'minas',\n",
              " 'de',\n",
              " 'água,',\n",
              " 'num',\n",
              " 'total',\n",
              " 'de',\n",
              " 'cerca',\n",
              " 'de',\n",
              " '2,5',\n",
              " 'hectares.']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wwJnb0bAcmWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dicionario_de_contagem(vocabulario, documento):\n",
        "  '''Recebe uma lista com o vocabulario e uma lista de tokens de um documento.\n",
        "  Retorna um dicionario com o numero de vezes que cada palavra do vocabulario\n",
        "  ocorre no documento.'''\n",
        "  dic = dict.fromkeys(vocabulario, 0)\n",
        "  for palavra in documento:\n",
        "    dic[palavra] += 1\n",
        "  return dic"
      ],
      "metadata": {
        "id": "6WVggm93XmxQ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p1_dic_cont = dicionario_de_contagem(Vocab, p1_tokens)\n",
        "p2_dic_cont = dicionario_de_contagem(Vocab, p2_tokens)\n",
        "\n",
        "print(p1_dic_cont, '\\n')\n",
        "print(p2_dic_cont)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "8CzY-7DOYnMU",
        "outputId": "fcc49479-3f50-4f35-c5d3-c76db6e6de8e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-0a457ec6c226>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp1_dic_cont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdicionario_de_contagem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mp2_dic_cont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdicionario_de_contagem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1_dic_cont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp2_dic_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-dc2f17ccf018>\u001b[0m in \u001b[0;36mdicionario_de_contagem\u001b[0;34m(vocabulario, documento)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulario\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mpalavra\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocumento\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpalavra\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'carrapatelo.'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H7uFm_OMTuhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_3A5-BzoU7_",
        "outputId": "1fcbcc2a-7c09-4dea-b1c8-5143042f38ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp (from openai)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.7 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solução da OpenAI não está com free trial, vamos experiementar com outra rede já treinada"
      ],
      "metadata": {
        "id": "-oiS4FQB_X1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import openai\n",
        "\n",
        "# openai.api_key = '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'\n",
        "\n",
        "# response = openai.Completion.create(\n",
        "#     engine='text-davinci-003',\n",
        "#     prompt='Hello, ChatGPT!',\n",
        "#     max_tokens=50,\n",
        "#     n=1,\n",
        "#     stop=None,\n",
        "#     temperature=0.7,\n",
        "# )\n",
        "\n",
        "# generated_response = response.choices[0].text.strip()\n",
        "# print(generated_response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "6QcfU0K9oQA_",
        "outputId": "691326d1-6206-4734-fc8a-65ae9bf02c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-03ed0e70c3c8>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'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'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m response = openai.Completion.create(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'text-davinci-003'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Hello, ChatGPT!'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         )\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             return (\n\u001b[0;32m--> 624\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    625\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    688\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s85PR-iM_goe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}